{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here are some imports that are used along this notebook\n",
    "import math\n",
    "import itertools\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "%matplotlib inline\n",
    "gt0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['protocol_type', 'service', 'flag']\n",
      "['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login', 'is_guest_login']\n"
     ]
    }
   ],
   "source": [
    "train20_nsl_kdd_dataset_path = \"C:/Users/user/MSDS/Capstone/NSL_KDD-master/NSL_KDD-master/KDDTrain+.txt\"\n",
    "#train_nsl_kdd_dataset_path = \"C:/Users/user/MSDS/Capstone/NSL_KDD-master/NSL_KDD-master/KDDTrain+.txt\"\n",
    "test_nsl_kdd_dataset_path = \"C:/Users/user/MSDS/Capstone/NSL_KDD-master/NSL_KDD-master/KDDTest+.txt\"\n",
    "\n",
    "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\",\"attrib43\"])\n",
    "\n",
    "\n",
    "nominal_inx = [1, 2, 3]\n",
    "binary_inx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_inx = list(set(range(41)).difference(nominal_inx).difference(binary_inx))\n",
    "\n",
    "nominal_cols = col_names[nominal_inx].tolist()\n",
    "binary_cols = col_names[binary_inx].tolist()\n",
    "numeric_cols = col_names[numeric_inx].tolist()\n",
    "print(nominal_cols)\n",
    "print(binary_cols)\n",
    "\n",
    "# Dictionary that contains mapping of various attacks to the four main categories\n",
    "attack_dict = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _label2(x):\n",
    "    if x['labels'] == 'normal':\n",
    "        return 'normal'\n",
    "    else:\n",
    "        return 'attack'\n",
    "\n",
    "def returnvalue(x):\n",
    "    return attack_dict.get(x['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_kdd_dataset_train = pd.read_csv(train20_nsl_kdd_dataset_path, index_col=None, names=col_names)\n",
    "df_kdd_dataset_train['label2'] = df_kdd_dataset_train.apply(_label2,axis=1)\n",
    "df_kdd_dataset_train['label3'] = df_kdd_dataset_train.apply(returnvalue,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_kdd_dataset_test = pd.read_csv(test_nsl_kdd_dataset_path, index_col=None, names=col_names)\n",
    "df_kdd_dataset_test['label2'] = df_kdd_dataset_test.apply(_label2,axis=1)\n",
    "df_kdd_dataset_test['label3'] = df_kdd_dataset_test.apply(returnvalue,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 45)\n",
      "(22544, 45)\n"
     ]
    }
   ],
   "source": [
    "print(df_kdd_dataset_train.shape)\n",
    "print(df_kdd_dataset_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148517, 45)\n"
     ]
    }
   ],
   "source": [
    "df_concatenate = [df_kdd_dataset_train,df_kdd_dataset_test]\n",
    "df_kdd_entire_set = pd.concat(df_concatenate)\n",
    "print(df_kdd_entire_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_kdd_entire_set[\"protocol_type\"] = le.fit_transform(df_kdd_entire_set[\"protocol_type\"])\n",
    "df_kdd_entire_set[\"service\"] = le.fit_transform(df_kdd_entire_set[\"service\"])\n",
    "df_kdd_entire_set[\"flag\"] = le.fit_transform(df_kdd_entire_set[\"flag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = \"C:/Users/user/MSDS/Capstone/NSL_KDD-master/NSL_KDD-master/output4.txt\"\n",
    "\n",
    "#Assuming res is a flat list\n",
    "df_kdd_entire_set.to_csv(csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x, n_classes):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "     \"\"\"\n",
    "    return np.eye(n_classes)[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ..., 0 1 0]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "y_label2 = df_kdd_entire_set['label2']\n",
    "le.fit(y_label2)\n",
    "y_label2 = le.transform(y_label2)\n",
    "print(y_label2)\n",
    "y_label2 = one_hot_encode(y_label2,2)\n",
    "print(y_label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 0 ..., 0 4 1]\n",
      "[[ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "y_label3 = df_kdd_entire_set['label3']\n",
    "le.fit(y_label3)\n",
    "y_label3 = le.transform(y_label3)\n",
    "print(y_label3)\n",
    "y_label3 = one_hot_encode(y_label3,5)\n",
    "print(y_label3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kdd_entire_set.drop('labels', axis=1, inplace=True)\n",
    "df_kdd_entire_set.drop('attrib43',axis=1,inplace=True)\n",
    "df_kdd_entire_set.drop('label2',axis=1,inplace=True)\n",
    "df_kdd_entire_set.drop('label3',axis=1,inplace=True)\n",
    "\n",
    "#enc = preprocessing.OneHotEncoder(categorical_features=[1, 2, 3])\n",
    "\n",
    "#enc.fit(df_kdd_entire_set)\n",
    "\n",
    "# 3. Transform\n",
    "#onehotlabels_train_test = enc.transform(df_kdd_entire_set).toarray()\n",
    "#print(onehotlabels_train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 41)\n",
      "(22544, 41)\n"
     ]
    }
   ],
   "source": [
    "train_size = df_kdd_dataset_train.shape[0]\n",
    "dff_kdd_train = df_kdd_entire_set[0:train_size]\n",
    "dff_kdd_test = df_kdd_entire_set[train_size:df_kdd_entire_set.shape[0]]\n",
    "print(dff_kdd_train.shape)\n",
    "print(dff_kdd_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = \"C:/Users/user/MSDS/Capstone/NSL_KDD-master/NSL_KDD-master/output.txt\"\n",
    "\n",
    "#Assuming res is a flat list\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in dff_kdd_train:\n",
    "        writer.writerow([val])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_train_transform = scaler.fit_transform(dff_kdd_train)\n",
    "df_test_transform = scaler.fit_transform(dff_kdd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 41)\n"
     ]
    }
   ],
   "source": [
    "print(df_train_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label2_train = y_label2[0:train_size]\n",
    "y_label2_test = y_label2[train_size:df_kdd_entire_set.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "(125973, 43)\n",
      "(22544, 43)\n"
     ]
    }
   ],
   "source": [
    "print(y_label2_train)\n",
    "print(y_label2_test)\n",
    "df_train_transform_with_label=np.c_[df_train_transform,y_label2_train]\n",
    "df_test_transform_with_label=np.c_[df_test_transform,y_label2_test]\n",
    "print(df_train_transform_with_label.shape)\n",
    "print(df_test_transform_with_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(i,batch_size,dataset):\n",
    "    start = i*batch_size\n",
    "    end = (i+1)*batch_size\n",
    "    return dataset[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder\n",
      "decoder\n",
      "Epoch: 0001 cost= 0.129345387\n",
      "Epoch: 0002 cost= 0.008430441\n",
      "Epoch: 0003 cost= 0.002446069\n",
      "Epoch: 0004 cost= 0.001504005\n",
      "Epoch: 0005 cost= 0.001232216\n",
      "Epoch: 0006 cost= 0.001076626\n",
      "Epoch: 0007 cost= 0.001002824\n",
      "Epoch: 0008 cost= 0.000896854\n",
      "Epoch: 0009 cost= 0.000865728\n",
      "Epoch: 0010 cost= 0.000826851\n",
      "Epoch: 0011 cost= 0.000828474\n",
      "Epoch: 0012 cost= 0.000899145\n",
      "Epoch: 0013 cost= 0.000778568\n",
      "Epoch: 0014 cost= 0.000793496\n",
      "Epoch: 0015 cost= 0.000766689\n",
      "Epoch: 0016 cost= 0.000777874\n",
      "Epoch: 0017 cost= 0.000733435\n",
      "Epoch: 0018 cost= 0.000735751\n",
      "Epoch: 0019 cost= 0.000706409\n",
      "Epoch: 0020 cost= 0.000710744\n",
      "Epoch: 0021 cost= 0.000742098\n",
      "Epoch: 0022 cost= 0.000701615\n",
      "Epoch: 0023 cost= 0.000707584\n",
      "Epoch: 0024 cost= 0.000678231\n",
      "Epoch: 0025 cost= 0.000693615\n",
      "Epoch: 0026 cost= 0.000690759\n",
      "Epoch: 0027 cost= 0.000684252\n",
      "Epoch: 0028 cost= 0.000684984\n",
      "Epoch: 0029 cost= 0.000666772\n",
      "Epoch: 0030 cost= 0.000667041\n",
      "Epoch: 0031 cost= 0.000646706\n",
      "Epoch: 0032 cost= 0.000659855\n",
      "Epoch: 0033 cost= 0.000649300\n",
      "Epoch: 0034 cost= 0.000667090\n",
      "Epoch: 0035 cost= 0.000657960\n",
      "Epoch: 0036 cost= 0.000649233\n",
      "Epoch: 0037 cost= 0.000652623\n",
      "Epoch: 0038 cost= 0.000683652\n",
      "Epoch: 0039 cost= 0.000645610\n",
      "Epoch: 0040 cost= 0.000654595\n",
      "Epoch: 0041 cost= 0.000639479\n",
      "Epoch: 0042 cost= 0.000637900\n",
      "Epoch: 0043 cost= 0.000635509\n",
      "Epoch: 0044 cost= 0.000663868\n",
      "Epoch: 0045 cost= 0.000622827\n",
      "Epoch: 0046 cost= 0.000612070\n",
      "Epoch: 0047 cost= 0.000597677\n",
      "Epoch: 0048 cost= 0.000580969\n",
      "Epoch: 0049 cost= 0.000510806\n",
      "Epoch: 0050 cost= 0.000467910\n",
      "Epoch: 0051 cost= 0.000432816\n",
      "Epoch: 0052 cost= 0.000445782\n",
      "Epoch: 0053 cost= 0.000436381\n",
      "Epoch: 0054 cost= 0.000422771\n",
      "Epoch: 0055 cost= 0.000399742\n",
      "Epoch: 0056 cost= 0.000384160\n",
      "Epoch: 0057 cost= 0.000394734\n",
      "Epoch: 0058 cost= 0.000389732\n",
      "Epoch: 0059 cost= 0.000374258\n",
      "Epoch: 0060 cost= 0.000369863\n",
      "Epoch: 0061 cost= 0.000461274\n",
      "Epoch: 0062 cost= 0.000386270\n",
      "Epoch: 0063 cost= 0.000413504\n",
      "Epoch: 0064 cost= 0.000410591\n",
      "Epoch: 0065 cost= 0.000373296\n",
      "Epoch: 0066 cost= 0.000414393\n",
      "Epoch: 0067 cost= 0.000400580\n",
      "Epoch: 0068 cost= 0.000381932\n",
      "Epoch: 0069 cost= 0.000365690\n",
      "Epoch: 0070 cost= 0.000373429\n",
      "Epoch: 0071 cost= 0.000370480\n",
      "Epoch: 0072 cost= 0.000379260\n",
      "Epoch: 0073 cost= 0.000347021\n",
      "Epoch: 0074 cost= 0.000357461\n",
      "Epoch: 0075 cost= 0.000383179\n",
      "Epoch: 0076 cost= 0.000361005\n",
      "Epoch: 0077 cost= 0.000394027\n",
      "Epoch: 0078 cost= 0.000393699\n",
      "Epoch: 0079 cost= 0.000344539\n",
      "Epoch: 0080 cost= 0.000367604\n",
      "Epoch: 0081 cost= 0.000375703\n",
      "Epoch: 0082 cost= 0.000370709\n",
      "Epoch: 0083 cost= 0.000361462\n",
      "Epoch: 0084 cost= 0.000365827\n",
      "Epoch: 0085 cost= 0.000362801\n",
      "Epoch: 0086 cost= 0.000355067\n",
      "Epoch: 0087 cost= 0.000390182\n",
      "Epoch: 0088 cost= 0.000361832\n",
      "Epoch: 0089 cost= 0.000375009\n",
      "Epoch: 0090 cost= 0.000357559\n",
      "Epoch: 0091 cost= 0.000359064\n",
      "Epoch: 0092 cost= 0.000356297\n",
      "Epoch: 0093 cost= 0.000353743\n",
      "Epoch: 0094 cost= 0.000346590\n",
      "Epoch: 0095 cost= 0.000348585\n",
      "Epoch: 0096 cost= 0.000330003\n",
      "Epoch: 0097 cost= 0.000354887\n",
      "Epoch: 0098 cost= 0.000363570\n",
      "Epoch: 0099 cost= 0.000380989\n",
      "Epoch: 0100 cost= 0.000335622\n",
      "Optimization Finished!\n",
      "0.000372606\n",
      "0.000901038\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 20 # 1st layer num features\n",
    "#n_hidden_2 = 25 # 2nd layer num features\n",
    "n_input = 41 # Number of features\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "autoencoder_op = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    #'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    #'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    #'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    #'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    #layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    print('encoder')\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    #layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    print('decoder')\n",
    "    return layer_1\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "    \n",
    "model_path = \"C:\\\\users\\\\user\\MSDS\\\\Capstone\\\\Models\"\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    total_batch = int(df_train_transform.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = get_next_batch(i,batch_size,df_train_transform)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs}) \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c))            \n",
    "    print(\"Optimization Finished!\")\n",
    "    #c2,dec_op2 = sess.run([cost,decoder_op], feed_dict={X: df_train_transform}) \n",
    "    save_path = saver.save(sess,model_path)\n",
    "    # model has been trained pass the training data with labels\n",
    "    cost_train,encoder_op_train_wo_label = sess.run([cost,encoder_op], feed_dict={X: df_train_transform}) \n",
    "    print(cost_train)\n",
    "    cost_test,encoder_op_test_wo_label = sess.run([cost,encoder_op], feed_dict={X: df_test_transform}) \n",
    "    print(cost_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder\n",
      "decoder\n",
      "Epoch: 0001 cost= 0.081370324\n",
      "Epoch: 0002 cost= 0.016366350\n",
      "Epoch: 0003 cost= 0.008956696\n",
      "Epoch: 0004 cost= 0.007210674\n",
      "Epoch: 0005 cost= 0.006335004\n",
      "Epoch: 0006 cost= 0.005588522\n",
      "Epoch: 0007 cost= 0.005190033\n",
      "Epoch: 0008 cost= 0.005108485\n",
      "Epoch: 0009 cost= 0.004837396\n",
      "Epoch: 0010 cost= 0.004741573\n",
      "Epoch: 0011 cost= 0.004685895\n",
      "Epoch: 0012 cost= 0.004579602\n",
      "Epoch: 0013 cost= 0.004525463\n",
      "Epoch: 0014 cost= 0.004457514\n",
      "Epoch: 0015 cost= 0.004291519\n",
      "Epoch: 0016 cost= 0.004184833\n",
      "Epoch: 0017 cost= 0.004149614\n",
      "Epoch: 0018 cost= 0.004133228\n",
      "Epoch: 0019 cost= 0.004233181\n",
      "Epoch: 0020 cost= 0.004048833\n",
      "Epoch: 0021 cost= 0.003995394\n",
      "Epoch: 0022 cost= 0.003888939\n",
      "Epoch: 0023 cost= 0.004012542\n",
      "Epoch: 0024 cost= 0.004017576\n",
      "Epoch: 0025 cost= 0.003990760\n",
      "Epoch: 0026 cost= 0.003833093\n",
      "Epoch: 0027 cost= 0.003887956\n",
      "Epoch: 0028 cost= 0.003836720\n",
      "Epoch: 0029 cost= 0.003866416\n",
      "Epoch: 0030 cost= 0.003661667\n",
      "Epoch: 0031 cost= 0.003752113\n",
      "Epoch: 0032 cost= 0.003746479\n",
      "Epoch: 0033 cost= 0.003714898\n",
      "Epoch: 0034 cost= 0.003682818\n",
      "Epoch: 0035 cost= 0.003701194\n",
      "Epoch: 0036 cost= 0.003686402\n",
      "Epoch: 0037 cost= 0.003898040\n",
      "Epoch: 0038 cost= 0.003906890\n",
      "Epoch: 0039 cost= 0.003764211\n",
      "Epoch: 0040 cost= 0.003746025\n",
      "Epoch: 0041 cost= 0.003617290\n",
      "Epoch: 0042 cost= 0.003549994\n",
      "Epoch: 0043 cost= 0.003619859\n",
      "Epoch: 0044 cost= 0.003751798\n",
      "Epoch: 0045 cost= 0.003571558\n",
      "Epoch: 0046 cost= 0.003565811\n",
      "Epoch: 0047 cost= 0.003768440\n",
      "Epoch: 0048 cost= 0.003653588\n",
      "Epoch: 0049 cost= 0.003556800\n",
      "Epoch: 0050 cost= 0.003645114\n",
      "Epoch: 0051 cost= 0.003537508\n",
      "Epoch: 0052 cost= 0.003644514\n",
      "Epoch: 0053 cost= 0.003434092\n",
      "Epoch: 0054 cost= 0.003601271\n",
      "Epoch: 0055 cost= 0.003668504\n",
      "Epoch: 0056 cost= 0.003645800\n",
      "Epoch: 0057 cost= 0.003581783\n",
      "Epoch: 0058 cost= 0.003566722\n",
      "Epoch: 0059 cost= 0.003630389\n",
      "Epoch: 0060 cost= 0.003565645\n",
      "Epoch: 0061 cost= 0.003472183\n",
      "Epoch: 0062 cost= 0.003476739\n",
      "Epoch: 0063 cost= 0.003612543\n",
      "Epoch: 0064 cost= 0.003566818\n",
      "Epoch: 0065 cost= 0.003539356\n",
      "Epoch: 0066 cost= 0.003488220\n",
      "Epoch: 0067 cost= 0.003447173\n",
      "Epoch: 0068 cost= 0.003492823\n",
      "Epoch: 0069 cost= 0.003395570\n",
      "Epoch: 0070 cost= 0.003478674\n",
      "Epoch: 0071 cost= 0.003474384\n",
      "Epoch: 0072 cost= 0.003440287\n",
      "Epoch: 0073 cost= 0.003587518\n",
      "Epoch: 0074 cost= 0.003450887\n",
      "Epoch: 0075 cost= 0.003528040\n",
      "Epoch: 0076 cost= 0.003379038\n",
      "Epoch: 0077 cost= 0.003363761\n",
      "Epoch: 0078 cost= 0.003580921\n",
      "Epoch: 0079 cost= 0.003483648\n",
      "Epoch: 0080 cost= 0.003369867\n",
      "Epoch: 0081 cost= 0.003518427\n",
      "Epoch: 0082 cost= 0.003411202\n",
      "Epoch: 0083 cost= 0.003399832\n",
      "Epoch: 0084 cost= 0.003436626\n",
      "Epoch: 0085 cost= 0.003377733\n",
      "Epoch: 0086 cost= 0.003465676\n",
      "Epoch: 0087 cost= 0.003321737\n",
      "Epoch: 0088 cost= 0.003444549\n",
      "Epoch: 0089 cost= 0.003394974\n",
      "Epoch: 0090 cost= 0.003508501\n",
      "Epoch: 0091 cost= 0.003370621\n",
      "Epoch: 0092 cost= 0.003487501\n",
      "Epoch: 0093 cost= 0.003458481\n",
      "Epoch: 0094 cost= 0.003372179\n",
      "Epoch: 0095 cost= 0.003376498\n",
      "Epoch: 0096 cost= 0.003390722\n",
      "Epoch: 0097 cost= 0.003564900\n",
      "Epoch: 0098 cost= 0.003491062\n",
      "Epoch: 0099 cost= 0.003433877\n",
      "Epoch: 0100 cost= 0.003349918\n",
      "Optimization Finished!\n",
      "0.00346786\n",
      "0.00528259\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 10 # 1st layer num features\n",
    "#n_hidden_2 = 25 # 2nd layer num features\n",
    "n_input = 20 # Number of features\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "autoencoder_op = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights_S2 = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    #'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    #'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases_S2 = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    #'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    #'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder_S2(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights_S2['encoder_h1']),biases_S2['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    #layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    print('encoder')\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder_S2(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights_S2['decoder_h1']),biases_S2['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    #layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    print('decoder')\n",
    "    return layer_1\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder_S2(X)\n",
    "decoder_op = decoder_S2(encoder_op)\n",
    "    \n",
    "model_path = \"C:\\\\users\\\\user\\MSDS\\\\Capstone\\\\Models\"\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    total_batch = int(df_train_transform.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = get_next_batch(i,batch_size,encoder_op_train_wo_label)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs}) \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c))            \n",
    "    print(\"Optimization Finished!\")\n",
    "    #c2,dec_op2 = sess.run([cost,decoder_op], feed_dict={X: df_train_transform}) \n",
    "    save_path = saver.save(sess,model_path)\n",
    "    # model has been trained pass the training data with labels\n",
    "    cost_train,encoder_op2_train_wo_label = sess.run([cost,encoder_op], feed_dict={X: encoder_op_train_wo_label}) \n",
    "    print(cost_train)\n",
    "    cost_test,encoder_op2_test_wo_label = sess.run([cost,encoder_op], feed_dict={X: encoder_op_test_wo_label}) \n",
    "    print(cost_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 10)\n",
      "(22544, 10)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_op2_train_wo_label.shape)\n",
    "print(encoder_op2_test_wo_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch_label(i,batch_size,dataset):\n",
    "    start = i*batch_size\n",
    "    end = (i+1)*batch_size\n",
    "    return dataset[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch_sftmax(i,batch_size):\n",
    "    start = i*batch_size\n",
    "    end = (i+1)*batch_size\n",
    "    return encoder_op2_train_wo_label[start:end,0:10],df_train_transform_with_label[start:end,41:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost= 0.184840858\n",
      "Epoch: 0020 cost= 0.175871925\n",
      "Epoch: 0030 cost= 0.173077309\n",
      "Epoch: 0040 cost= 0.171578419\n",
      "Epoch: 0050 cost= 0.170582937\n",
      "Epoch: 0060 cost= 0.169857987\n",
      "Epoch: 0070 cost= 0.169304316\n",
      "Epoch: 0080 cost= 0.168868759\n",
      "Epoch: 0090 cost= 0.168519005\n",
      "Epoch: 0100 cost= 0.168233908\n",
      "Epoch: 0110 cost= 0.167998927\n",
      "Epoch: 0120 cost= 0.167803657\n",
      "Epoch: 0130 cost= 0.167640408\n",
      "Epoch: 0140 cost= 0.167503299\n",
      "Epoch: 0150 cost= 0.167387780\n",
      "Epoch: 0160 cost= 0.167290191\n",
      "Epoch: 0170 cost= 0.167207603\n",
      "Epoch: 0180 cost= 0.167137613\n",
      "Epoch: 0190 cost= 0.167078230\n",
      "Epoch: 0200 cost= 0.167027813\n",
      "Epoch: 0210 cost= 0.166984965\n",
      "Epoch: 0220 cost= 0.166948550\n",
      "Epoch: 0230 cost= 0.166917576\n",
      "Epoch: 0240 cost= 0.166891232\n",
      "Epoch: 0250 cost= 0.166868808\n",
      "Epoch: 0260 cost= 0.166849729\n",
      "Epoch: 0270 cost= 0.166833481\n",
      "Epoch: 0280 cost= 0.166819649\n",
      "Epoch: 0290 cost= 0.166807867\n",
      "Epoch: 0300 cost= 0.166797835\n",
      "Epoch: 0310 cost= 0.166789292\n",
      "Epoch: 0320 cost= 0.166782015\n",
      "Epoch: 0330 cost= 0.166775813\n",
      "Epoch: 0340 cost= 0.166770529\n",
      "Epoch: 0350 cost= 0.166766023\n",
      "Epoch: 0360 cost= 0.166762190\n",
      "Epoch: 0370 cost= 0.166758923\n",
      "Epoch: 0380 cost= 0.166756138\n",
      "Epoch: 0390 cost= 0.166753762\n",
      "Epoch: 0400 cost= 0.166751739\n",
      "Epoch: 0410 cost= 0.166750017\n",
      "Epoch: 0420 cost= 0.166748549\n",
      "Epoch: 0430 cost= 0.166747292\n",
      "Epoch: 0440 cost= 0.166746226\n",
      "Epoch: 0450 cost= 0.166745319\n",
      "Epoch: 0460 cost= 0.166744541\n",
      "Epoch: 0470 cost= 0.166743881\n",
      "Epoch: 0480 cost= 0.166743316\n",
      "Epoch: 0490 cost= 0.166742834\n",
      "Epoch: 0500 cost= 0.166742422\n",
      "Epoch: 0510 cost= 0.166742080\n",
      "Epoch: 0520 cost= 0.166741782\n",
      "Epoch: 0530 cost= 0.166741526\n",
      "Epoch: 0540 cost= 0.166741312\n",
      "Epoch: 0550 cost= 0.166741132\n",
      "Epoch: 0560 cost= 0.166740970\n",
      "Epoch: 0570 cost= 0.166740841\n",
      "Epoch: 0580 cost= 0.166740726\n",
      "Epoch: 0590 cost= 0.166740629\n",
      "Epoch: 0600 cost= 0.166740547\n",
      "Epoch: 0610 cost= 0.166740473\n",
      "Epoch: 0620 cost= 0.166740417\n",
      "Epoch: 0630 cost= 0.166740364\n",
      "Epoch: 0640 cost= 0.166740320\n",
      "Epoch: 0650 cost= 0.166740287\n",
      "Epoch: 0660 cost= 0.166740250\n",
      "Epoch: 0670 cost= 0.166740228\n",
      "Epoch: 0680 cost= 0.166740203\n",
      "Epoch: 0690 cost= 0.166740186\n",
      "Epoch: 0700 cost= 0.166740174\n",
      "Epoch: 0710 cost= 0.166740156\n",
      "Epoch: 0720 cost= 0.166740143\n",
      "Epoch: 0730 cost= 0.166740132\n",
      "Epoch: 0740 cost= 0.166740126\n",
      "Epoch: 0750 cost= 0.166740121\n",
      "Epoch: 0760 cost= 0.166740110\n",
      "Epoch: 0770 cost= 0.166740108\n",
      "Epoch: 0780 cost= 0.166740102\n",
      "Epoch: 0790 cost= 0.166740097\n",
      "Epoch: 0800 cost= 0.166740096\n",
      "Epoch: 0810 cost= 0.166740094\n",
      "Epoch: 0820 cost= 0.166740093\n",
      "Epoch: 0830 cost= 0.166740087\n",
      "Epoch: 0840 cost= 0.166740087\n",
      "Epoch: 0850 cost= 0.166740084\n",
      "Epoch: 0860 cost= 0.166740088\n",
      "Epoch: 0870 cost= 0.166740084\n",
      "Epoch: 0880 cost= 0.166740081\n",
      "Epoch: 0890 cost= 0.166740084\n",
      "Epoch: 0900 cost= 0.166740082\n",
      "Epoch: 0910 cost= 0.166740080\n",
      "Epoch: 0920 cost= 0.166740078\n",
      "Epoch: 0930 cost= 0.166740077\n",
      "Epoch: 0940 cost= 0.166740079\n",
      "Epoch: 0950 cost= 0.166740079\n",
      "Epoch: 0960 cost= 0.166740079\n",
      "Epoch: 0970 cost= 0.166740077\n",
      "Epoch: 0980 cost= 0.166740076\n",
      "Epoch: 0990 cost= 0.166740076\n",
      "Epoch: 1000 cost= 0.166740079\n",
      "Optimization Finished!\n",
      "accuracy 0.942107\n",
      "accuracy 0.756388\n"
     ]
    }
   ],
   "source": [
    "## Now we build the softmax regressor\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 1000\n",
    "batch_size = 1000\n",
    "display_step = 10\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 10]) # 118 features\n",
    "y = tf.placeholder(tf.float32, [None, 2]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([10, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(df_train_transform_with_label.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = get_next_batch_sftmax(i,batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    print(\"accuracy\", sess.run(accuracy, feed_dict={x: encoder_op2_train_wo_label[:,0:10], y: df_train_transform_with_label[:,41:43]}))\n",
    "    #prediction=tf.argmax(y,1)\n",
    "    #print (\"predictions\", prediction.eval(feed_dict={x: df_train_transform_with_label[0:118]}, session=sess))\n",
    "    print(\"accuracy\", sess.run(accuracy, feed_dict={x: encoder_op2_test_wo_label[:,0:10], y: df_test_transform_with_label[:,41:43]}))\n",
    "    #print(\"accuracy\", sess.run(accuracy, feed_dict={x: df_train_transform_with_label[:,0:118], y: df_train_transform_with_label[:,118:120]}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dec_op_test_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch_sftmax_wostl(i,batch_size):\n",
    "    start = i*batch_size\n",
    "    end = (i+1)*batch_size\n",
    "    return df_train_transform_with_label[start:end,0:41],df_train_transform_with_label[start:end,41:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we build the softmax regressor\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.3\n",
    "training_epochs = 1000\n",
    "batch_size = 1000\n",
    "display_step = 10\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 41]) # 118 features\n",
    "y = tf.placeholder(tf.float32, [None, 2]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([41, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(df_train_transform_with_label.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = get_next_batch_sftmax_wostl(i,batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    print(\"accuracy\", sess.run(accuracy, feed_dict={x: df_train_transform_with_label[:,0:41], y: df_train_transform_with_label[:,41:43]}))\n",
    "    #prediction=tf.argmax(y,1)\n",
    "    #print (\"predictions\", prediction.eval(feed_dict={x: df_train_transform_with_label[0:118]}, session=sess))\n",
    "    print(\"accuracy\", sess.run(accuracy, feed_dict={x: df_test_transform_with_label[:,0:41], y: df_test_transform_with_label[:,41:43]}))\n",
    "    #print(\"accuracy\", sess.run(accuracy, feed_dict={x: df_train_transform_with_label[:,0:118], y: df_train_transform_with_label[:,118:120]}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
